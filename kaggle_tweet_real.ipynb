{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_tweet_real",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dht7166/NLP_tweet/blob/master/kaggle_tweet_real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS6-W3vYeIM5",
        "colab_type": "text"
      },
      "source": [
        "# Kaggle: Tweet Real or Not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n9_cj7aeOLA",
        "colab_type": "code",
        "outputId": "62e06e13-3d80-4532-8b84-0615b6e7d979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "mydir = 'drive/My Drive/kaggle_nlp_tweet/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mms8IRJhnA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import keras\n",
        "from keras.layers import Embedding,LSTM, Dense, Dropout\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import  Constant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GuTRVY1e5NO",
        "colab_type": "text"
      },
      "source": [
        "## Read and Prepare the database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiW-QH1Eekah",
        "colab_type": "code",
        "outputId": "d320ad0e-b690-4a13-873c-944c89b25c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "db = pd.read_csv(os.path.join(mydir,'train.csv'))\n",
        "db"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7608</th>\n",
              "      <td>10869</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7609</th>\n",
              "      <td>10870</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7610</th>\n",
              "      <td>10871</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7611</th>\n",
              "      <td>10872</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Police investigating after an e-bike collided ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7612</th>\n",
              "      <td>10873</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7613 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id keyword  ...                                               text target\n",
              "0         1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1         4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2         5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3         6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4         7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "...     ...     ...  ...                                                ...    ...\n",
              "7608  10869     NaN  ...  Two giant cranes holding a bridge collapse int...      1\n",
              "7609  10870     NaN  ...  @aria_ahrary @TheTawniest The out of control w...      1\n",
              "7610  10871     NaN  ...  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...      1\n",
              "7611  10872     NaN  ...  Police investigating after an e-bike collided ...      1\n",
              "7612  10873     NaN  ...  The Latest: More Homes Razed by Northern Calif...      1\n",
              "\n",
              "[7613 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAH-cGkMe1hT",
        "colab_type": "code",
        "outputId": "aee460c1-eec1-4c79-d5e5-30bac4ef3b7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# We can combine keyword, location into the actual text and ignore punctuation\n",
        "\n",
        "tweet = []\n",
        "target = []\n",
        "max_length = 0\n",
        "for _, row in db.iterrows():\n",
        "  target.append(row['target'])\n",
        "  text = row['text']\n",
        "  text = text + ' ' + str(row['keyword'])\n",
        "  text = text + ' ' + str(row['location'])\n",
        "  # We would also remove any links or tags\n",
        "  list_word = text.split()\n",
        "  new_list = [word for word in list_word if ('http' not in word) and ('@' not in word) ]\n",
        "  max_length = max([max_length,len(new_list)-1])\n",
        "  tweet.append(' '.join(new_list))\n",
        "\n",
        "token = Tokenizer()\n",
        "token.fit_on_texts(tweet)\n",
        "\n",
        "X = token.texts_to_sequences(tweet)\n",
        "X = pad_sequences(X,maxlen=max_length)\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(target)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"MAX STRING LENGTH \", max_length)\n",
        "print(\"VOCAB SIZE\",len(token.word_index)+1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX STRING LENGTH  34\n",
            "VOCAB SIZE 17711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIStD0Fwkh_N",
        "colab_type": "text"
      },
      "source": [
        "## Create da model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncSoFxeZgHg-",
        "colab_type": "code",
        "outputId": "6df812f0-be22-4d36-b017-8188c72f25ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# Build the model\n",
        "model = keras.models.Sequential()\n",
        "# We have word 2 vec pretrained so no need to retrain embedding layer\n",
        "model.add(Embedding(len(token.word_index)+1,300,input_length = max_length,name = \"Embedding\"))\n",
        "model.add(LSTM(300,name=\"LSTM\"))\n",
        "model.add(Dense(50,activation='relu',name=\"Dense\"))\n",
        "model.add(Dense(1,activation='sigmoid',name = \"Out\"))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Embedding (Embedding)        (None, 34, 300)           5313300   \n",
            "_________________________________________________________________\n",
            "LSTM (LSTM)                  (None, 300)               721200    \n",
            "_________________________________________________________________\n",
            "Dense (Dense)                (None, 50)                15050     \n",
            "_________________________________________________________________\n",
            "Out (Dense)                  (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 6,049,601\n",
            "Trainable params: 6,049,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfhuyxtUlD4g",
        "colab_type": "code",
        "outputId": "7a0967bd-c497-4861-ccd6-a25b767d552f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "ckpt_val = keras.callbacks.ModelCheckpoint(os.path.join(mydir,'best_val_acc.h5'),\n",
        "                                                 monitor='val_acc', verbose=1, save_best_only=True)\n",
        "ckpt_acc = keras.callbacks.ModelCheckpoint(os.path.join(mydir,'best_acc.h5'),\n",
        "                                                 monitor='acc', verbose=1, save_best_only=True)\n",
        "\n",
        "model.fit(X,Y,epochs=20,validation_split=0.2, callbacks=[ckpt_val,ckpt_acc])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 6090 samples, validate on 1523 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "6090/6090 [==============================] - 69s 11ms/step - loss: 0.5260 - acc: 0.7366 - val_loss: 0.5132 - val_acc: 0.7636\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.76362, saving model to drive/My Drive/kaggle_nlp_tweet/best_val_acc.h5\n",
            "\n",
            "Epoch 00001: acc improved from -inf to 0.73662, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 2/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.2920 - acc: 0.8851 - val_loss: 0.5701 - val_acc: 0.7479\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00002: acc improved from 0.73662 to 0.88506, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 3/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.1585 - acc: 0.9429 - val_loss: 0.6058 - val_acc: 0.7505\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00003: acc improved from 0.88506 to 0.94286, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 4/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0858 - acc: 0.9672 - val_loss: 1.0055 - val_acc: 0.7387\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00004: acc improved from 0.94286 to 0.96716, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 5/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0598 - acc: 0.9778 - val_loss: 0.7939 - val_acc: 0.7577\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00005: acc improved from 0.96716 to 0.97783, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 6/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0524 - acc: 0.9778 - val_loss: 0.9803 - val_acc: 0.7177\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00006: acc did not improve from 0.97783\n",
            "Epoch 7/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0343 - acc: 0.9846 - val_loss: 1.4596 - val_acc: 0.7045\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00007: acc improved from 0.97783 to 0.98456, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 8/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0394 - acc: 0.9828 - val_loss: 1.3683 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00008: acc did not improve from 0.98456\n",
            "Epoch 9/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0348 - acc: 0.9844 - val_loss: 1.2290 - val_acc: 0.7334\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00009: acc did not improve from 0.98456\n",
            "Epoch 10/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0345 - acc: 0.9842 - val_loss: 1.2383 - val_acc: 0.7315\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00010: acc did not improve from 0.98456\n",
            "Epoch 11/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0274 - acc: 0.9875 - val_loss: 1.2877 - val_acc: 0.7617\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00011: acc improved from 0.98456 to 0.98752, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 12/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0322 - acc: 0.9851 - val_loss: 1.5590 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00012: acc did not improve from 0.98752\n",
            "Epoch 13/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0304 - acc: 0.9862 - val_loss: 1.5393 - val_acc: 0.6993\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00013: acc did not improve from 0.98752\n",
            "Epoch 14/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0342 - acc: 0.9859 - val_loss: 1.2652 - val_acc: 0.7157\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00014: acc did not improve from 0.98752\n",
            "Epoch 15/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0244 - acc: 0.9892 - val_loss: 1.5346 - val_acc: 0.7026\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00015: acc improved from 0.98752 to 0.98916, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 16/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0174 - acc: 0.9911 - val_loss: 1.5759 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00016: acc improved from 0.98916 to 0.99113, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 17/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0161 - acc: 0.9918 - val_loss: 1.6937 - val_acc: 0.7144\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00017: acc improved from 0.99113 to 0.99179, saving model to drive/My Drive/kaggle_nlp_tweet/best_acc.h5\n",
            "Epoch 18/20\n",
            "6090/6090 [==============================] - 67s 11ms/step - loss: 0.0156 - acc: 0.9916 - val_loss: 1.7227 - val_acc: 0.7203\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00018: acc did not improve from 0.99179\n",
            "Epoch 19/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0148 - acc: 0.9915 - val_loss: 1.9327 - val_acc: 0.7209\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00019: acc did not improve from 0.99179\n",
            "Epoch 20/20\n",
            "6090/6090 [==============================] - 66s 11ms/step - loss: 0.0153 - acc: 0.9915 - val_loss: 1.9493 - val_acc: 0.7236\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.76362\n",
            "\n",
            "Epoch 00020: acc did not improve from 0.99179\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f289097a048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLvlO54imepo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(os.path.join(mydir,'nlp.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd_XEz7ZmTlE",
        "colab_type": "text"
      },
      "source": [
        "## Predict on test set and write to output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu5HQqlkmfc0",
        "colab_type": "code",
        "outputId": "cfce0f9e-e2bd-4d50-8e01-775beff6011c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "test = pd.read_csv(os.path.join(mydir,'test.csv'))\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>10861</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3259</th>\n",
              "      <td>10865</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3260</th>\n",
              "      <td>10868</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>10874</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>10875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3263 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id keyword location                                               text\n",
              "0         0     NaN      NaN                 Just happened a terrible car crash\n",
              "1         2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2         3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3         9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4        11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
              "...     ...     ...      ...                                                ...\n",
              "3258  10861     NaN      NaN  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...\n",
              "3259  10865     NaN      NaN  Storm in RI worse than last hurricane. My city...\n",
              "3260  10868     NaN      NaN  Green Line derailment in Chicago http://t.co/U...\n",
              "3261  10874     NaN      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...\n",
              "3262  10875     NaN      NaN  #CityofCalgary has activated its Municipal Eme...\n",
              "\n",
              "[3263 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULRb7-sEmTEh",
        "colab_type": "code",
        "outputId": "4517bbba-c3bf-45a0-969e-8b9f7954da73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet_test = []\n",
        "ID = []\n",
        "max_length_test = 0\n",
        "for _, row in test.iterrows():\n",
        "  ID.append(row['id'])\n",
        "  text = row['text']\n",
        "  text = text + ' ' + str(row['keyword'])\n",
        "  text = text + ' ' + str(row['location'])\n",
        "  # We would also remove any links or tags\n",
        "  list_word = text.split()\n",
        "  new_list = [word for word in list_word if ('http' not in word) and ('@' not in word) ]\n",
        "  max_length_test = max([max_length_test,len(new_list)-1])\n",
        "  tweet_test.append(' '.join(new_list))\n",
        "\n",
        "print(\"MAX STRING LENGTH \", max_length_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAX STRING LENGTH  35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGboqZlnn_f6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict\n",
        "X_test = token.texts_to_sequences(tweet_test)\n",
        "X_test = pad_sequences(X_test,maxlen=max_length)\n",
        "X_test = np.asarray(X_test)\n",
        "\n",
        "Y_pred = model.predict(X_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFWAFjIzrvGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = [[ID[i],int(Y_pred[i]+0.5)] for i in range(len(ID))]\n",
        "result = pd.DataFrame(result, columns = ['id', 'target'])\n",
        "result\n",
        "result.to_csv(os.path.join(mydir,'submission_1.csv'),index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}